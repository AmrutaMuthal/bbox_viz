{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import requests \n",
    "import tqdm\n",
    "import tarfile\n",
    "from clint.textui import progress\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    \n",
    "    with open(save_path, 'wb') as fd:\n",
    "        total_length = int(r.headers.get('content-length'))\n",
    "        \n",
    "        for chunk in progress.bar(r.iter_content(chunk_size=chunk_size),\n",
    "                                  expected_size=(total_length/1024) + 1):\n",
    "            fd.write(chunk)\n",
    "            fd.flush()\n",
    "                                  \n",
    "    return\n",
    "\n",
    "def extract_file(fname,path='.'):\n",
    "    if fname.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(fname, \"r:gz\")\n",
    "        tar.extractall(path=path)\n",
    "        tar.close()\n",
    "    elif fname.endswith(\"tar\"):\n",
    "        tar = tarfile.open(fname, \"r:\")\n",
    "        tar.extractall(path=path)\n",
    "        tar.close()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files\n",
    "\n",
    "img_url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2010/VOCtrainval_03-May-2010.tar\"\n",
    "annot_url = \"http://roozbehm.info/pascal-parts/trainval.tar.gz\"\n",
    "\n",
    "save_path_img = \"img_zip\"\n",
    "save_path_annot = \"annot_zip\"\n",
    "\n",
    "curr_path = os.getcwd()\n",
    "path = Path('D://meronym//datasets')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = os.path.join(path.parent,'datasets','PASCAL-VOC',save_path_img)\n",
    "os.mkdir(save_loc)\n",
    "download_url(img_url,os.path.join(save_loc,'images.tar'))\n",
    "\n",
    "save_loc = os.path.join(path.parent,'datasets','PASCAL-VOC',save_path_annot)\n",
    "os.mkdir(save_loc)\n",
    "download_url(annot_url,os.path.join(save_loc,'annotations.tar.gz'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = os.path.join(path.parent,'datasets','PASCAL-VOC',save_path_annot)\n",
    "extract_file(os.path.join(save_loc,'annotations.tar.gz'),path = save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = os.path.join(path.parent,'datasets','PASCAL-VOC',save_path_img)\n",
    "extract_file(os.path.join(save_loc+'images.tar'),path = save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class_list = os.listdir(save_loc+'\\\\VOCdevkit\\\\VOC2010\\\\ImageSets\\\\Main')\n",
    "# class_list = [file.split('_')[0] for file in class_list]\n",
    "# class_list = list(filter(lambda x: ('val' not in x)and('train' not in x),\n",
    "#                          class_list))\n",
    "# class_set = set(class_list)\n",
    "class_set = set(['aeroplane','bicycle','bird','cat','cow','dog','horse','motorbike','person','sheep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "anno_destination = os.path.join(path.parent,'datasets','PASCAL-VOC','xybb-objects')\n",
    "anno_source = os.path.join(path.parent,'datasets','PASCAL-VOC',\n",
    "                                        save_path_annot,\n",
    "                                        'Annotations_Part')\n",
    "img_source = os.path.join(path.parent,'datasets','PASCAL-VOC',\n",
    "                                       save_path_img,\n",
    "                                       'VOCdevkit','VOC2010','JPEGImages')\n",
    "\n",
    "save_loc = os.path.join(path.parent,'datasets','PASCAL-VOC',save_path_img)\n",
    "for ob_class in class_set:\n",
    "                                       \n",
    "    new_dir = os.path.join(anno_destination,ob_class)\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.mkdir(new_dir)\n",
    "    \n",
    "    if not os.path.exists(new_dir+'\\\\bbox'):\n",
    "        os.mkdir(new_dir+'\\\\bbox')\n",
    "        \n",
    "    if not os.path.exists(new_dir+'\\\\mask'):\n",
    "        os.mkdir(new_dir+'\\\\mask')\n",
    "        \n",
    "    if not os.path.exists(new_dir+'\\\\rgb'):\n",
    "        os.mkdir(new_dir+'\\\\rgb')\n",
    "    \n",
    "    if not os.path.exists(new_dir+'\\\\numpy'):\n",
    "        os.mkdir(new_dir+'\\\\numpy')\n",
    "    \n",
    "#for ob_class in class_set:\n",
    "#    print(ob_class)\n",
    "    \n",
    "img_list = pd.read_csv(os.path.join(save_loc,'VOCdevkit','VOC2010','ImageSets','Main','train_trainval.txt'),\n",
    "                      header=None,index_col=None,sep='\\t')\n",
    "\n",
    "\n",
    "img_list = img_list[0].str.split(' ',expand=True)\n",
    "img_list.columns = ['img','flag','drop']\n",
    "img_list.drop(['drop'],axis=1,inplace=True)\n",
    "ctr=100\n",
    "for image in img_list.img[100:300]:\n",
    "    ctr+=1\n",
    "    print(ctr)\n",
    "    mat = scipy.io.loadmat(os.path.join(anno_source,image+'.mat'))\n",
    "    img_file = os.path.join(img_source,image+'.jpg')\n",
    "    img_dest = os.path.join(path.parent,'datasets','PASCAL-VOC','scene',image+'.jpg')\n",
    "    \n",
    "    if not os.path.exists(img_dest):\n",
    "        shutil.move(img_file,img_dest)\n",
    "    label_list = []\n",
    "    annotation_dict = {}\n",
    "    item_idx = 0\n",
    "    label_wise_dict = {}\n",
    "    for label in class_set:\n",
    "        label_wise_dict[label] = {}\n",
    "    #print(len(mat['anno'][0][0][1][0]),'annotations')\n",
    "    for annotation in mat['anno'][0][0][1][0]:\n",
    "        #annotation_dict = {}\n",
    "        label = annotation[0][0]\n",
    "        if label not in class_set:\n",
    "            continue\n",
    "\n",
    "#         if os.path.exists(os.path.join(anno_destination,label,'bbox',image+'.json')):\n",
    "            \n",
    "#             try:\n",
    "#                 with open(os.path.join(anno_destination,label,'bbox',image+'.json')) as fp:\n",
    "#                     annotation_dict = json.load(fp)\n",
    "#             except:\n",
    "#                 print(image,label)\n",
    "#                 continue\n",
    "        annotation_dict = label_wise_dict[label]\n",
    "        item_idx = len(annotation_dict.keys())\n",
    "            #item_idx = max(map(int,annotation_dict.keys()))+1\n",
    "\n",
    "        annotation_dict[item_idx] = {}\n",
    "        annotation_dict[item_idx]['mask'] = annotation[2].tolist()\n",
    "        annotation_dict[item_idx]['bbox'] = np.append(np.min(np.where(annotation[2]==1),\n",
    "                                                                   axis=1),\n",
    "                                                   np.max(np.where(annotation[2]==1),\n",
    "                                                                   axis=1)).tolist()\n",
    "        annotation_dict[item_idx]['parts'] = {}\n",
    "        parts_dict = annotation_dict[item_idx]['parts'] \n",
    "\n",
    "        if (len(annotation[3])>0)and (len(annotation[3][0])>0):\n",
    "            for part in annotation[3][0]:\n",
    "\n",
    "                part_label = part[0][0]\n",
    "                parts_dict[part_label] = {}\n",
    "                parts_dict[part_label]['mask'] = part[1].tolist()\n",
    "                parts_dict[part_label]['bbox'] = np.append(np.min(np.where(part[1]==1),\n",
    "                                                                          axis=1),\n",
    "                                                       np.max(np.where(part[1]==1),\n",
    "                                                              axis=1)).tolist()\n",
    "\n",
    "        #item_idx+=1\n",
    "\n",
    "\n",
    "\n",
    "#         if len(annotation_dict.keys())>0:\n",
    "#             with open(os.path.join(anno_destination,label,'bbox',image+'.json'), 'w') as fp:\n",
    "#                 json.dump(annotation_dict, fp)\n",
    "        \n",
    "    for label in class_set:\n",
    "        if len(label_wise_dict[label].keys())>0:\n",
    "            with open(os.path.join(anno_destination,label,'bbox',image+'.json'), 'w') as fp:\n",
    "                json.dump(label_wise_dict[label], fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_labels = {'head':1, 'leye':2, 'reye':3, 'beak':4, 'torso':5, 'neck':6, 'lwing':7, 'rwing':8, 'lleg':9, 'lfoot':10, 'rleg':11, 'rfoot':12, 'tail':13}\n",
    "\n",
    "cat_labels = {'head':1, 'leye':2, 'reye':3, 'lear':4, 'rear':5, 'nose':6, 'torso':7, 'neck':8, 'lfleg':9, 'lfpa':10, 'rfleg':11, 'rfpa':12, 'lbleg':13, 'lbpa':14, 'rbleg':15, 'rbpa':16, 'tail':17}\n",
    "\n",
    "cow_labels = {'head':1, 'leye':2, 'reye':3, 'lear':4, 'rear':5, 'muzzle':6, 'lhorn':7, 'rhorn':8, 'torso':9, 'neck':10, 'lfuleg':11, 'lflleg':12, 'rfuleg':13, 'rflleg':14, 'lbuleg':15, 'lblleg':16, 'rbuleg':17, 'rblleg':18, 'tail':19}\n",
    "\n",
    "dog_labels = {'head':1, 'leye':2, 'reye':3, 'lear':4, 'rear':5, 'nose':6, 'torso':7, 'neck':8, 'lfleg':9, 'lfpa':10, 'rfleg':11, 'rfpa':12, 'lbleg':13, 'lbpa':14, 'rbleg':15, 'rbpa':16, 'tail':17, 'muzzle':18}\n",
    "\n",
    "horse_labels = {'head':1, 'leye':2, 'reye':3, 'lear':4, 'rear':5, 'muzzle':6, 'lfho':7, 'rfho':8, 'torso':9, 'neck':10, 'lfuleg':11, 'lflleg':12, 'rfuleg':13, 'rflleg':14, 'lbuleg':15, 'lblleg':16, 'rbuleg':17, 'rblleg':18, 'tail':19, 'lbho':20, 'rbho':21}\n",
    "9\n",
    "bottle_labels = {'cap':1, 'body':2}\n",
    "\n",
    "person_labels = {'head':1, 'leye':2,  'reye':3, 'lear':4, 'rear':5, 'lebrow':6, 'rebrow':7,  'nose':8,  'mouth':9,  'hair':10, 'torso':11, 'neck': 12, 'llarm': 13, 'luarm': 14, 'lhand': 15, 'rlarm':16, 'ruarm':17, 'rhand': 18, 'llleg': 19, 'luleg':20, 'lfoot':21, 'rlleg':22, 'ruleg':23, 'rfoot':24}\n",
    "\n",
    "bus_labels = { 'frontside':1, 'leftside':2, 'rightside':3, 'backside':4, 'roofside':5, 'leftmirror':6, 'rightmirror':7, 'fliplate':8, 'bliplate':9  }\n",
    "for ii in range(0,10):\n",
    "    bus_labels['door_{}'.format(ii+1)] = 10+ii\n",
    "for ii in range(0,10):\n",
    "    bus_labels['wheel_{}'.format(ii+1)] = 20+ii\n",
    "for ii in range(0,10):\n",
    "    bus_labels['headlight_{}'.format(ii+1)] = 30+ii\n",
    "for ii in range(0,20):\n",
    "    bus_labels['window_{}'.format(ii+1)] = 40+ii\n",
    "\n",
    "car_labels = { 'frontside':1, 'leftside':2, 'rightside':3, 'backside':4, 'roofside':5, 'leftmirror':6, 'rightmirror':7, 'fliplate':8, 'bliplate':9  }\n",
    "for ii in range(0,3):\n",
    "    car_labels['door_{}'.format(ii+1)] = 10+ii\n",
    "for ii in range(0,4):\n",
    "    car_labels['wheel_{}'.format(ii+1)] = 13+ii\n",
    "for ii in range(0,6):\n",
    "    car_labels['headlight_{}'.format(ii+1)] = 17+ii\n",
    "for ii in range(0,7):\n",
    "    car_labels['window_{}'.format(ii+1)] = 23+ii\n",
    "\n",
    "aeroplane_labels = {'body': 1, 'stern': 2, 'lwing': 3, 'rwing':4, 'tail':5}\n",
    "for ii in range(1, 10):\n",
    "    aeroplane_labels['engine_{}'.format(ii)] = 5+ii\n",
    "for ii in range(1, 10):\n",
    "    aeroplane_labels['wheel_{}'.format(ii)] = 14+ii\n",
    "\n",
    "motorbike_labels = {'fwheel': 1, 'bwheel': 2, 'handlebar': 3, 'saddle': 4}\n",
    "for ii in range(0,10):\n",
    "    motorbike_labels['headlight_{}'.format(ii+1)] = 5+ii\n",
    "motorbike_labels['body']=15\n",
    "\n",
    "bicycle_labels = {'fwheel': 1, 'bwheel': 2, 'saddle': 3, 'handlebar': 4, 'chainwheel': 5}\n",
    "for ii in range(0,10):\n",
    "    bicycle_labels['headlight_{}'.format(ii+1)] = 6+ii\n",
    "bicycle_labels['body']=16\n",
    "\n",
    "train_labels = {'head':1,'hfrontside':2,'hleftside':3,'hrightside':4,'hbackside':5,'hroofside':6}\n",
    "for ii in  range(0,10):\n",
    "    train_labels['headlight_{}'.format(ii+1)] = 7 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['coach_{}'.format(ii+1)] = 17 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['cfrontside_{}'.format(ii+1)] = 27 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['cleftside_{}'.format(ii+1)] = 37 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['crightside_{}'.format(ii+1)] = 47 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['cbackside_{}'.format(ii+1)] = 57 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['croofside_{}'.format(ii+1)] = 67 + ii\n",
    "\n",
    "sheep_labels = cow_labels\n",
    "\n",
    "part_labels = {'bird': bird_labels, 'cat': cat_labels, 'cow': cow_labels, 'dog': dog_labels, 'sheep': sheep_labels, 'horse':horse_labels, 'car':car_labels, 'bus':bus_labels, 'bicycle':bicycle_labels, 'motorbike':motorbike_labels, 'person':person_labels,'aeroplane':aeroplane_labels, 'train':train_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9c2b945e7d20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclass_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'class_set' is not defined"
     ]
    }
   ],
   "source": [
    "class_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path.parent,'datasets','PASCAL-VOC','part_label.json'), 'w') as fp:\n",
    "    json.dump(part_labels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_minAreaRect(img, rect):\n",
    "\n",
    "    # rotate img\n",
    "    angle = rect[2]\n",
    "    rows,cols = img.shape[0], img.shape[1]\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),angle,1)\n",
    "    img_rot = cv2.warpAffine(img,M,(cols,rows))\n",
    "    \n",
    "    # rotate bounding box\n",
    "    rect0 = (rect[0], rect[1], 0.0) \n",
    "    box = cv2.boxPoints(rect0)\n",
    "    pts = np.int0(cv2.transform(np.array([box]), M))[0]    \n",
    "    pts[pts < 0] = 0\n",
    "\n",
    "    # crop\n",
    "    img_crop = img_rot[pts[1][1]:pts[0][1], \n",
    "                       pts[1][0]:pts[2][0]]\n",
    "\n",
    "    return img_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sheep 0\n",
      "sheep 566 715\n"
     ]
    }
   ],
   "source": [
    "# convert to numpy\n",
    "anno_destination = os.path.join(path.parent,'datasets','PASCAL-VOC','xybb-objects')\n",
    "anno_source = os.path.join(path.parent,'datasets','PASCAL-VOC',\n",
    "                                        save_path_annot,\n",
    "                                        'Annotations_Part')\n",
    "img_source = os.path.join(path.parent,'datasets','PASCAL-VOC',\n",
    "                                       save_path_img,\n",
    "                                       'VOCdevkit','VOC2010','JPEGImages')\n",
    "\n",
    "class_set = ( 'sheep', )\n",
    "for ob_class in class_set:\n",
    "    \n",
    "    part_mapping = part_labels[ob_class]\n",
    "    \n",
    "    img_list = os.listdir(os.path.join(anno_destination,ob_class,'bbox'))\n",
    "    labels = [] #np.zeros([24,len(annotation_dict.keys())])\n",
    "    masks = []\n",
    "    bboxes = []\n",
    "    obboxes = []\n",
    "    obboxes_or = []\n",
    "    ob_masks = []\n",
    "    max_labels = 24\n",
    "    missing_count = 0 \n",
    "    print(ob_class,missing_count)\n",
    "    for file in img_list:\n",
    "        \n",
    "        try:\n",
    "            with open(os.path.join(anno_destination,ob_class,'bbox',file)) as fp:\n",
    "                annotation_dict = json.load(fp)\n",
    "        except:\n",
    "            continue\n",
    "        img_dest = os.path.join(path.parent,'datasets','PASCAL-VOC','scene',file.split('.')[0]+'.jpg')\n",
    "        image_arr = cv2.imread(img_dest)\n",
    "        for ct, idx in enumerate(annotation_dict.keys()):\n",
    "            anno = annotation_dict[idx]\n",
    "            p_list = anno['parts'].keys()\n",
    "            x_shift,y_shift,imx,imy = anno['bbox']\n",
    "            #x_shift,y_shift = 0,0\n",
    "            label_arr = np.array([part_mapping[x] for x in p_list])\n",
    "            padding = max_labels-label_arr.shape[0]\n",
    "            label_arr = np.pad(label_arr,(0,padding),'constant',constant_values=(0))\n",
    "            labels.append(label_arr)\n",
    "            \n",
    "            bb_array = []\n",
    "            obb_or_array = []\n",
    "            obb_array = []\n",
    "            mask_array = []\n",
    "            ob_mask_array = []\n",
    "            img = np.zeros(image_arr.shape)\n",
    "            if len(p_list)>0:\n",
    "                missed = False\n",
    "                for p_key in p_list:\n",
    "                    x0,y0,x1,y1 = anno['parts'][p_key]['bbox']\n",
    "                    #bb_mask = bb_array[0][0]:bb_array[0][2],bb_array[0][1]:bb_array[0][3]\n",
    "                    if (x0==x1):\n",
    "                        if x1==(imx-1):\n",
    "                            x0-=1\n",
    "                        else:\n",
    "                            x1+=1\n",
    "                    \n",
    "                    if (y0==y1):\n",
    "                        if y1==(imy-1):\n",
    "                            y0-=1\n",
    "                        else:\n",
    "                            y1+=1\n",
    "                        \n",
    "                    part = image_arr[x0:x1,y0:y1]    \n",
    "                    mask = part*np.expand_dims(np.array(anno['parts'][p_key]['mask'])[x0:x1,y0:y1],-1)\n",
    "                    mask = cv2.resize(mask.astype('uint8'),(64,64))\n",
    "                    mask_array.append(mask)\n",
    "                    contours,_ = cv2.findContours(np.uint8(np.array(anno['parts'][p_key]['mask'])),\n",
    "                               cv2.RETR_EXTERNAL , cv2.CHAIN_APPROX_NONE)\n",
    "                    rect = cv2.minAreaRect(contours[0])\n",
    "                    c,d,theta = rect\n",
    "                    cx,cy = c\n",
    "                    w,h = d\n",
    "                    obbox = cv2.boxPoints(rect)\n",
    "                    obbox_cropped = crop_minAreaRect(image_arr, rect)\n",
    "                    \n",
    "                    bb_array.append([x0,y0,x1,y1])\n",
    "                    obb_or_array.append([cx,cy,w,h,theta])\n",
    "                    if  min(list(obbox_cropped.shape))==0:\n",
    "                        missed = True\n",
    "                        obbox_cropped = mask\n",
    "                        #print(anno['parts'][p_key]['bbox'],obbox,file,ob_class,p_key)\n",
    "                        #x_min,y_min,x_max,y_max = anno['parts'][p_key]['bbox']  \n",
    "                        #np.zeros((64,64,3))\n",
    "                        #d = np.dot(np.array([x1-x0,0]),\n",
    "                        #           np.array([x1-x0,y1-y0]))/max((x1-x0)**2+(y1-y0)**2,1)\n",
    "                        #obb_array.append(list(np.append([x0,y0,x1,y1],[d]))) \n",
    "                               \n",
    "                    else:\n",
    "                        obbox_cropped = cv2.resize(obbox_cropped.astype('uint8'),(64,64))\n",
    "                    d = np.dot(obbox[1]-obbox[0],obbox[2]-obbox[0])/max(np.sum((obbox[2]-obbox[0])**2),1) \n",
    "                    ox0,oy0,ox1,oy1 = obbox[[0,2],:].flatten()\n",
    "                    if (np.max(obbox[:,0])>imx) or (np.max(obbox[:,1])>imy):\n",
    "\n",
    "                        imx = max(np.max(obbox[:,[0]]),imx)\n",
    "                        imy = max(np.max(obbox[:,[1]]),imy)\n",
    "\n",
    "                    if (np.min(obbox[:,0])<x_shift) or (np.min(obbox[:,1])<y_shift):\n",
    "\n",
    "                        x_shift= min(np.min(obbox[:,[0]]),x_shift)\n",
    "                        y_shift= min(np.min(obbox[:,[1]]),y_shift)\n",
    "            \n",
    "                            \n",
    "                    #obb_array.append(list(np.append([ox0,oy0,ox1,oy1],[d])))\n",
    "                        \n",
    "                    ob_mask_array.append(obbox_cropped)\n",
    "                if  missed:\n",
    "                    missing_count+=1\n",
    "                \n",
    "                imx-=x_shift\n",
    "                imy-=y_shift\n",
    "                        \n",
    "                bb_array = np.pad(bb_array,((0,padding),(0,0)),'constant',constant_values=(0))\n",
    "                obb_or_array = np.array(obb_or_array)               \n",
    "                obb_or_array[:,0] = (obb_or_array[:,0]-x_shift)\n",
    "                obb_or_array[:,1] = (obb_or_array[:,1]-y_shift)\n",
    "                \n",
    "                obb_or_array = np.pad(obb_or_array,((0,padding),(0,0)),'constant',constant_values=(0))\n",
    "                mask_array = np.pad(mask_array,((0,padding),(0,0),(0,0),(0,0)),'constant',constant_values=(0))\n",
    "#                 obb_array = np.array(obb_array)\n",
    "                \n",
    "                \n",
    "#                 obb_array[:,0] = (obb_array[:,0]-x_shift)\n",
    "#                 obb_array[:,2] = (obb_array[:,2]-x_shift)\n",
    "#                 obb_array[:,1] = (obb_array[:,1]-y_shift)\n",
    "#                 obb_array[:,3] = (obb_array[:,3]-y_shift)\n",
    "                \n",
    "#                 obb_array = np.pad(obb_array,((0,padding),(0,0)),'constant',constant_values=(0))\n",
    "                \n",
    "#                 ob_mask_array = np.pad(ob_mask_array,((0,padding),(0,0),(0,0),(0,0)),'constant',constant_values=(0))\n",
    "                bboxes.append(bb_array)\n",
    "                masks.append(mask_array)\n",
    "                obboxes.append(obb_array)\n",
    "                ob_masks.append(ob_mask_array)\n",
    "                obboxes_or.append(obb_or_array)\n",
    "            \n",
    "            \n",
    "                  \n",
    "    print(ob_class, missing_count,len(labels))\n",
    "    \n",
    "#     with open(os.path.join(anno_destination,ob_class,'numpy',ob_class+'_part_separated_labels_un'), 'wb') as f:\n",
    "#         pickle.dump(np.array(labels), f)\n",
    "\n",
    "#     with open(os.path.join(anno_destination,ob_class,'numpy',ob_class+'_part_separated_bbx_un'), 'wb') as f:\n",
    "#         pickle.dump(np.array(bboxes), f)\n",
    "    \n",
    "#     with open(os.path.join(anno_destination,ob_class,'numpy',ob_class+'_part_separated_obbx_un'), 'wb') as f:\n",
    "#         pickle.dump(np.array(obboxes), f)\n",
    "    \n",
    "    with open(os.path.join(anno_destination,ob_class,'numpy',ob_class+'_part_separated_obbx_or_un'), 'wb') as f:\n",
    "        pickle.dump(np.array(obboxes_or), f)\n",
    "    \n",
    "#     with open(os.path.join(anno_destination,ob_class,'numpy',ob_class+'_part_separated_ob_masks_un'), 'wb') as f:\n",
    "#         pickle.dump(np.array(ob_masks), f)\n",
    "\n",
    "#     with open(os.path.join(anno_destination,ob_class,'numpy',ob_class+'_part_separated_masks_un'), 'wb') as f:\n",
    "#         pickle.dump(np.array(masks), f)\n",
    "\n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cat 0\n",
    "cat 965 1041\n",
    "dog 0\n",
    "dog 1211 1286\n",
    "cow 0\n",
    "cow 361 443\n",
    "horse 0\n",
    "horse 521 553\n",
    "motorbike 0\n",
    "motorbike 340 540\n",
    "bird 0\n",
    "bird 664 839\n",
    "bicycle 0\n",
    "bicycle 462 553\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bicycle 0\n",
    "bicycle 462 553\n",
    "bird 0\n",
    "bird 664 839\n",
    "cat 0\n",
    "cat 965 1041\n",
    "cow 0\n",
    "cow 361 443\n",
    "dog 0\n",
    "dog 1211 1286\n",
    "horse 0\n",
    "horse 521 553\n",
    "motorbike 0\n",
    "motorbike 340 540\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
